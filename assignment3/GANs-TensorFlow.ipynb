{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "So far in CS231N, all the applications of neural networks that we have explored have been **discriminative models** that take an input and are trained to produce a labeled output. This has ranged from straightforward classification of image categories to sentence generation (which was still phrased as a classification problem, our labels were in vocabulary space and weâ€™d learned a recurrence to capture multi-word labels). In this notebook, we will expand our repetoire, and build **generative models** using neural networks. Specifically, we will learn how to build models which generate novel images that resemble a set of training images.\n",
    "\n",
    "### What is a GAN?\n",
    "\n",
    "In 2014, [Goodfellow et al.](https://arxiv.org/abs/1406.2661) presented a method for training generative models called Generative Adversarial Networks (GANs for short). In a GAN, we build two different neural networks. Our first network is a traditional classification network, called the **discriminator**. We will train the discriminator to take images, and classify them as being real (belonging to the training set) or fake (not present in the training set). Our other network, called the **generator**, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.\n",
    "\n",
    "We can think of this back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "where $x \\sim p_\\text{data}$ are samples from the input data, $z \\sim p(z)$ are the random noise samples, $G(z)$ are the generated images using the neural network generator $G$, and $D$ is the output of the discriminator, specifying the probability of an input being real. In [Goodfellow et al.](https://arxiv.org/abs/1406.2661), they analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from $G$.\n",
    "\n",
    "To optimize this minimax game, we will aternate between taking gradient **descent** steps on the objective for $G$, and gradient **ascent** steps on the objective for $D$:\n",
    "1. update the **generator** ($G$) to minimize the probability of the __discriminator making the correct choice__. \n",
    "2. update the **discriminator** ($D$) to maximize the probability of the __discriminator making the correct choice__.\n",
    "\n",
    "While these updates are useful for analysis, they do not perform well in practice. Instead, we will use a different objective when we update the generator: maximize the probability of the **discriminator making the incorrect choice**. This small change helps to allevaiate problems with the generator gradient vanishing when the discriminator is confident. This is the standard update used in most GAN papers, and was used in the original paper from [Goodfellow et al.](https://arxiv.org/abs/1406.2661). \n",
    "\n",
    "In this assignment, we will alternate the following updates:\n",
    "1. Update the generator ($G$) to maximize the probability of the discriminator making the incorrect choice on generated data:\n",
    "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "2. Update the discriminator ($D$), to maximize the probability of the discriminator making the correct choice on real and generated data:\n",
    "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "### What else is there?\n",
    "Since 2014, GANs have exploded into a huge research area, with massive [workshops](https://sites.google.com/site/nips2016adversarial/), and [hundreds of new papers](https://github.com/hindupuravinash/the-gan-zoo). Compared to other approaches for generative models, they often produce the highest quality samples but are some of the most difficult and finicky models to train (see [this github repo](https://github.com/soumith/ganhacks) that contains a set of 17 hacks that are useful for getting models working). Improving the stabiilty and robustness of GAN training is an open research question, with new papers coming out every day! For a more recent tutorial on GANs, see [here](https://arxiv.org/abs/1701.00160). There is also some even more recent exciting work that changes the objective function to Wasserstein distance and yields much more stable results across model architectures: [WGAN](https://arxiv.org/abs/1701.07875), [WGAN-GP](https://arxiv.org/abs/1704.00028).\n",
    "\n",
    "\n",
    "GANs are not the only way to train a generative model! For other approaches to generative modeling check out the [deep generative model chapter](http://www.deeplearningbook.org/contents/generative_models.html) of the Deep Learning [book](http://www.deeplearningbook.org). Another popular way of training neural networks as generative models is Variational Autoencoders (co-discovered [here](https://arxiv.org/abs/1312.6114) and [here](https://arxiv.org/abs/1401.4082)). Variational autoencoders combine neural networks with variational inference to train deep generative models. These models tend to be far more stable and easier to train but currently don't produce samples that are as pretty as GANs.\n",
    "\n",
    "Example pictures of what you should expect (yours might look slightly different):\n",
    "\n",
    "![caption](gan_outputs_tf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# A bunch of utility functions\n",
    "\n",
    "def show_images(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params():\n",
    "    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n",
    "    param_count = np.sum([np.prod(x.get_shape().as_list()) for x in tf.global_variables()])\n",
    "    return param_count\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "answers = np.load('gan-checks-tf.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    " GANs are notoriously finicky with hyperparameters, and also require many training epochs. In order to make this assignment approachable without a GPU, we will be working on the MNIST dataset, which is 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). This was one of the first datasets used to train convolutional neural networks and it is fairly easy -- a standard CNN model can easily exceed 99% accuracy. \n",
    " \n",
    "\n",
    "**Heads-up**: Our MNIST wrapper returns images as vectors. That is, they're size (batch, 784). If you want to treat them as images, we have to resize them to (batch,28,28) or (batch,28,28,1). They are also type np.float32 and bounded [0,1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST(object):\n",
    "    def __init__(self, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct an iterator object over the MNIST data\n",
    "        \n",
    "        Inputs:\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        train, _ = tf.keras.datasets.mnist.load_data()\n",
    "        X, y = train\n",
    "        X = X.astype(np.float32)/255\n",
    "        X = X.reshape((X.shape[0], -1))\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAEkCAYAAAAVXTsLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXmgVVP7xz9XCUnRYCiSNFGURHippMGshEKlpIiKlKGQMaLMaTJGRKJQModEvCRDMoQyhF4NkhLS74/7++51z7n33HvOvWeftc+9z+efU2dc6+5z9v6uZ32f58nZsmULhmEYhj+28j0AwzCMso6diA3DMDxjJ2LDMAzP2InYMAzDM3YiNgzD8IydiA3DMDxjJ2LDMAzP2InYMAzDM3YiNgzD8Ez5VJ6ck5NT6tLwtmzZkqN/2/yyj7I0Pyj9cyzt80uEKWLDMAzP2InYMAzDM3YiNgzD8IydiA3DMDxjJ2LDMAzP2InYMAzDMynZ14z0cuCBBwIwYMAAAHr27AnAww8/DMDdd98NwMKFCz2MzjCMTJGTSoeOMD1+5cqVA6BKlSr5HtOJqmLFigA0bNgQgAsuuACAMWPGAHD66acD8OeffwIwatQorr322kI/14eHsVmzZgC89tprAFSuXLnA5/32228AVKtWrdiflQ0ezaOOOgqARx99FIDWrVsD8MUXXxT52ijO78orrwQIvntbbZW78GzTpg0Ab7zxRtLvZT7icNlhhx0AqFSpEgDHHXccADVq1ADgtttuA2DTpk3F/gzzERuGYWQBGQtN1K5dG4AKFSoAcNhhhwFw+OGHA7DjjjsC0KVLlyLf64cffgDgrrvuAqBz584A/P777wB89NFHQGrKI1McfPDBPPXUU4BT/1qVaPx//fUX4JTwIYccArgQhR5PN61atQo+c8aMGaF8RkEcdNBBAPz3v//N2GeGQa9evQC47LLLAPj3339jHrf+kNGgTp06wTE69NBDAWjSpEmBz91tt90AGDRoUKhjMkVsGIbhmdAVcXw8tKAYcLJIYSgGt379esDFFn/66ScA1qxZAyQXYwwbxbWbN28OwJQpU4KrbDxfffUVALfccgsAjz/+OADz588H3LxvuummUMbapk0b6tevD2RGESt2utdeewGw5557ApCTU2RILZJo/Ntuu63nkRSPli1bAtC9e/cgTt+4ceOY5wwdOhSAFStWAG5FO2XKFADefffdjIw1FRo1agTARRddBMCZZ57JdtttB7jv2vfffw+4Vek+++wDwGmnnQbAuHHjAPj8889DGaMpYsMwDM+Eroi/++47AFatWgUkp4h1VV27di0ARx55JOBio4888kjaxxkWEydOBJyjozCkmrWDqxi3dtv333//EEbo6NmzJ++8806on5EXrQz69u0LOFUVluoIi3bt2gEwcODAmPs1j+OPPx6AX375JbMDS5KuXbsCcOeddwJQvXr1QCm+/vrrgHMRjB49Oua1ep4e79atW+jjLQqdY26++WbAzU8OibxoFdqxY0cAtt56a8Adu+rVq8fchoUpYsMwDM+ErohXr14NwCWXXAI4dfDhhx8CzvkgFi1aRPv27QH4448/ABenuvDCC8MebtpQsoZ8iXnjnlK6zz33HOB80Iq76W+jWHfbtm3zvUcYKGabKe67776Y/0udZAuKjz744INA/tWe1OPy5cszO7AiKF8+92ffokULAO69917A7We8+eabXH/99QC89dZbAGyzzTYATJs2DYAOHTrEvOf7778f8qiTRy6qc845J+Fzvv76a4DgXKMYcb169UIeXcGYIjYMw/BMxnzEM2fOBJx7QruTTZs2BaBPnz5ArjqUEhaLFy8GoF+/fhkZa0mQS+Tll18GXNacPKRz5swJ4sXamZYbQgrxf//7H+D80HKLSF0rlpyu1GfFnnfZZZe0vF+yxCtI/c2yhbPOOguAmjVrxtyvuKpS1aNG9+7dgfwrEv39u3btyrp162IeU5w1XgnL0z958uRQxlocTj311ALvX7ZsGZDrV5ePWEpYyC2RaUwRG4ZheCbjRX/ir7SqpyD69u3LE088AeTPTIoyDRo0AFwsXGrv119/BZzHefLkyYH/efbs2TG3RSHv45AhQ4BcP2Q6OPbYY2PeP2ykvOUfFj/++GNGPr+kaAf97LPPBtz3VC6fG264wc/AikBx3+HDhwNulSaPrFZm8b9RgCuuuKLA91TGmVZxUUAuHK2gX3rpJQCWLl0KwMqVKxO+NtOrQmGK2DAMwzPey2Bec801gHMZtG7dOvBl6koWZbSbLOeD1KVi4CptqV3ldKhO1e1IF6pmBy4eHyb6W0l9fPnll4D7m0WZOnXqBLVC4lHZ0rlz52ZySEUyYsQIwClh+fFffPFFwNXG2LhxY/AaZQcqJqzvnJw7Uv3PPPNMqGMvDnIf6dySCqo9kWlMERuGYXjGuyKWQ0JxnYULFwa+RikLqcl77rkHiFYVqwMOOABwSlicdNJJQDQrwBVGOiugyTFy9NFHA263Pn7nXbFLxVijzNFHH50vw/HVV18FXGZaVFBFw/PPPx9wvxsp4U6dOhX4unr16gX1W7RSFdOnTwdcPZRsZNCgQWy//fYFPrbffvvF/P/tt98GCD3j1PuJWMhg3atXr8Ag36NHj5hb/fFkC9IGmE9UOFpLNp1403kCVqJFJjYvq1atWujjshtqvgoj7b777kBumVNtImrcWvIqdV1FtpVY8MEHH6Rt/GGhk9aoUaOC+5TsIBtb/Mazb1RyNj49VxtsO++8MwC9e/cG4MQTTwRyS0IqzV4nb90qDT3eYhpFlKCy7777AnD11VcDsaIp0W9L4Q39bTZv3hzqWC00YRiG4ZnIKGIxY8aMINVValOtdG688UbAlRscOXIk4Mf2pFRtJXBIMTz77LNp/yxdrfUZixYtSuv7S7Fu2bKFCRMmAG5jJx4ty6WI//nnHwA2bNgAwGeffcYDDzwAuJCSVgcqeqMkAG1cRrnIT506dQAK3KD75ptvgOgW89GmnKxlKszz7bffAolDfCtWrAgsbCrMJBum0vKjiAr2KFyoY6Y56Hu+YsWKINSgsJnUs9Bq7eSTTwZc2CmspgymiA3DMDwTOUUM8OmnnwKuKPMJJ5wAuOIq5557LkBQxFyFOzKJ1JzicDKJKxmlJMgSF2+/UXr4sGHDSvwZedFmzvLly4MWVolQWVOlrC9ZsgSABQsWFPk5MthLmUlRRplEbY8gNl4cRbT5qfj2rFmzALcPoH0ZWdAeeughILdQl5oSSE3q/1FEv0Gp26effjrmcTVx1e9n/vz5wd9A98W3StJ3VE0Y4r/3JWkmWhCmiA3DMDwTSUUsdEVXIXgVKVH8plWrVoArnK5iKz7QFbIkTg4pYaWaKl1aMdVbb70VcC2i0o0KaYeFYv0iUWJEFFDsP95qB05BRqEVVzLIrSKVVxStWrUKClJpJRDF1YtiwlK8+r2IOXPmAC7RRueTGjVq8PzzzwPOrqbYr2x5UsiyocrO98orrwDut6JStVCyvRtTxIZhGJ6JpCLWzvwpp5wCuHbrUsLis88+A3ILWfumJG4JqS9d0VVyUMqrS5cuJRxdNMlEg9LiovT6nXbaKeb+BQsW0KtXLw8jyhzbbbddPqdOlGLE5cqVA1wikBqaytt8+eWXA27MUsIqhD927NjAWSGHVv/+/QGXRKZkJO2ZyBsvr3Xekq0qpRlfxCoVTBEbhmF4JjKKWIVnBgwYEHj3dt111wKfqywXxWN9lMuUj1a32plOpZ3T4MGDAbjqqqsAVzpT8SgVDDIyT7Vq1YD8361x48aFFqOPCkqBjipy30gJy8MuN5VWM4cccgjgsuOOOeYYIFfxX3fddYBzYsUXiJeP+oUXXoi5VVOHM844I3iufsclwRSxYRiGZ7wpYqldXWEGDBgAuEymglCmljLqwshiS5b4HHzNR81QlV22atUqwF2de/ToEdRrUH0GeRSlRFSou7SiVYSK6SfjQc4UUkiJGqmqCExpRq3lo4rKegrFjLXHIv99okag11xzTeAPTrWGxNSpU2Nu04UpYsMwDM9kTBGrCLgqIY0dOxaARo0aJXyN/I9qSy4XQRRbKOmqrCw1OR0Ua1IWYF6krrRTG3+lL61oFZFIdfpAzhVVk9N3TP5SlWCNal2JdFK3bl3fQyiUn3/+GXC+aPnvtdIU8grLVaWsuGXLloVeTS1VovNLMAzDKKOEpoiVyz1x4kTAKY6irrZvv/12kEGmmGneFi5RQdWbVEhdXmehmHF8M8JVq1YF/sZUHBalEbWlUY0Dn6iIerxTR5X9tENfFpg3b15Ga2CnijJq5VRq3rw54Oq9aH9GWW9hVUxLJ6aIDcMwPJNWRdyyZctg5/Lggw8GoFatWoW+Rh5AuQ1uvPHGrKj+r/oP8jzLw6g6EfGonun48eODtt5lFbkmjGjy6aefBhlnWsHuvffegKtt7BM1mVUNGt1mM2k9EXfu3JnOnTsX+JjSkVWKTwXFFYbIhn5lBaGkEllmitM5tqygIiynnnqq55HkR8XptYF6+OGH+xyOd9SEQYW2ZBkdOHAg4H7PRnqw0IRhGIZnclLpiJyTkxOd9slpYsuWLcE62eaXfZSl+UHm5qiiN9OmTQOcrU9F15U2nI4wYlk7hgVhitgwDMMzpojL0NXY5pd9+FLEQspYMWKVi1Sp2nTEisvaMSwIU8SGYRieMUVchq7GNr/sw7cizgRl7RgWhCliwzAMz6SkiA3DMIz0Y4rYMAzDM3YiNgzD8ExKKc6lPZBu88s+ytL8oPTPsbTPLxGmiA3DMDxjJ2LDMAzP2InYMAzDM3YiNgzD8IydiA3DMDxjJ2LDMAzPhNY8NFXUSmjQoEF8+umnABx//PEALF++3Nu4DCPdvPrqq0Buy6i2bdt6Hg3su+++gPu99evXL2iK++GHH8Y894477gCyoyFnNuH9RFynTh0AunfvDuR2jd1nn30AaNSoEZDdJ+IGDRoAsPXWWwO5HWjHjRsHJN8h95lnngGgW7duQDR/BJrfYYcdFrTZ+c9//uNzSJHj9ttvB3L/RgAPP/ywz+EEfRbHjBkDQKVKlYLH1KNO3zmhE/TcuXMzMcQyg4UmDMMwPONdEasr7JtvvgnAiSee6HM4JaZx48YA9OrVC3CNMrfaKveaV7NmzUAJJ1twSX+TCRMmAHDRRRcBsG7duvQMOg1UqVIFyFVKP//8MwC77rorQPD/ssqoUaMAOO+88wD4+++/ARei8MWTTz4JwHXXXQfEKuJEqFVS165dAXjppZdCGl3ZwhSxYRiGZ7wrYjUfzOY4cF5uuukmAI499ti0v3fPnj0BuP/++wGYP39+2j8jHUgJmyLO5ZBDDgFcHP2tt94CXGNOX6xevRqAq6++GoBbb70VgIoVK/Ldd98BULt27ZjX7LjjjgAcffTRQNlRxHvuuScA2223HQCnn3464FpHAcyePRtwjVVTwRSxYRiGZ7wrYl1hmzZt6nkk6eHll18G8ivilStXArlqVvHieNeEdtNbt24d9jBDJSenyGJTWUerVq0AuOKKKwI1JEWZCD2vSZMmAHz99dcADB06NKxhFgvtPSiG3bRp0yL3H8aOHRv6uHzSrl07AE4++WTAHUvthRS0v6OVT3EwRWwYhuEZ74q4YsWKQP5YFMBBBx0EwOeffw5kRxx5/PjxAMycOTPmfu2UFxYvVetyJbTUrFkz5nG95/vvv5+ewYaE1MK2227reSTpY9KkSQDUr18/SIBQrDcRw4cPB6BatWoA9O3bF4CPPvoorGGWiBtuuAHIVf3NmjUr9LkVKlTIxJAyxn333QfAfvvtB7hzTzy///47AI8++iiQ66ueOnUqAH/++WexP98UsWEYhme8K+IVK1YA8NBDDwFwzTXXBI/p32vXrgWyIy71zz//APD999+n/NqOHTsCsNNOOxX4+A8//ADApk2bijm6zNKiRQsAFixY4HkkJWfDhg1ArtovSulLTWqnXXsBUV8hTJ8+HchV+nJDSCHGI/V8yimnZGZwaUarFLmczj77bMDF/T/44APAecC1St24cSNA4CpJF6aIDcMwPONdEYvrr78eiFXEZQXl8yuGKK9iPCNGjMjYmFJFK4Hffvst2FlWvYJsRt9LKcMlS5YkjPFuv/32AFx22WWA2//QikCKM6qceeaZQK5rQk6PRBQVH486V111FQB9+vQB4O677wZy4+MA69evz+h4TBEbhmF4JjKKWGy11VZJVyXLVqQ8Lr/8cgDq1asHuMyreBYtWgQ450UUURx/3rx5QTnFbGaPPfYA3CpFin/AgAFBfZR4brvtNsDVF9H+R1Sr0Km64YwZMwD3PSxfvujTwrPPPhvewNKMViZaqfTo0SOo16Iqci+++CJQMudDSTBFbBiG4ZnIKeJ///036apkUUT1lXv06AG4DJ28HH744UDi6mvKapJifv755wG3Y2uEh2KjUonVq1cHXAzxjTfeyPcaZcqp4p4YOXJkWMNMC6r7vddeewHJKWExePBgAAYOHJj+gaWZK6+8EnCKeNq0aYErxJcCjidyJ+JsRT9gLdkKSlBJlnnz5gEuiSBbkUUoyujko8YEKqgUn4Z+6KGHAjBs2LAgBFG1alXAhSKU2q2C7xMnTgx9/CVBF5tLL70UgJtvvhlIzma32267hTewNDNs2DDACZ+pU6dG5gQsLDRhGIbhGVPEaUaqqLDCN4mK/ghtdh1zzDEAzJkzJ51DzBjZUORf1kGluEo16dgsXboUcMkpLVq04KSTTgKgVq1agFOH2sRTckC2cNdddwHw1VdfAa4QF7gVg5KplIafTbz33nuAO4Zjx44Nwnwq0uUbU8SGYRieiZwiLsi+phKEUU5xVgpkmzZtABdzTMYWI1N5Nmx8FMXcuXOzwr6mVj8PPvgg4KyBsuGdccYZAKxZswZwRdNbt24dKCuteqSitbGn9HZ9F1T+MuoUtPLSHGVtU1JRfBp3FApytWzZEnCdp9VkVyvLQYMGAbnJHEqu0WtUWMwXpogNwzA8k5OKVSwnJyd0X9nmzZsT2rr2339/AD777LO0fd6WLVuCYG4m5lcQSgletWpVzP0nnHACULIYcabn16VLl6AppeJwKhsZhmoq7vxee+01wCk6FbGRQo5Hc5g4cWLgoIhXxOKxxx4DXGurkpB3fv//mRn9jm6zzTZA/hWdFGT79u0BV5CqOBT3GCo2P2vWLMA5lWStmzJlSszztWL55ZdfgvuOOOIIAN5+++3UB54k8cewIEwRG4ZheCZyMeIJEyZw7rnnFvhYv379ANdOvrSg8pelAaUCg1OMUlVR4plnngFce/iiypZKTeUthqP2OdofECVRh1FDK4V45Lf2OdeFCxcCzsmhhI14JSwuvPDC4N+vvPIKkP/Y+cIUsWEYhmcip4h9714miwr0dOjQAXAxx1TSkNV2+84770zz6PzxzDPPBMdQRWW0gjn//PO9jSueZP/mit8re65y5cqBC2LatGnhDC5NKLNRcW+19NFtYSj+qlVoPFpJ+ET+Z6Uw6/+6FfJH169fH8jdq1C2XVFNUjOFKWLDMAzPRE4R33333YGfNr6wuGI8KsDiw5+pgj0qIK1dYxVOKSrWWLVqVY499ljAlU1UmT4hVR21fPhkUUEVZZ5dfPHFPodTIqTi+/fvD8DKlStp27atzyEljZSh3DcNGjQAXHnOH3/8EXDZgwceeGDwPNWfiM+kk59a7+ETtTmSB/yAAw4A8hfaUuux2bNnA7lFmjTnqGCK2DAMwzORU8QAixcvBqBu3box90ehYLyy++JbyUhBqN12Itq3b0/z5s2B/P7T119/HYDx48cDrmh1tqL5KcMpm5C/+JxzzgHcXCZNmpQ1rgitHLVak/9Z37Nly5YBzpcvT+0OO+wQvIfmrbj/1VdfDURrtTZmzBjfQygxpogNwzA8E0lFrDq8im1lA4ohpsLKlSsBeO655wAXA4+S2igJii+qWpnq32YDqsolZSxvqhRhNqCmpe+88w4AjzzyCADjxo0DXBMD3RaEam0os9AIB1PEhmEYnomkIlbMasmSJYBr6RIF1A5Hzo6zzjorqdfJ4bFhw4Z8HTiikt2TLk477TQANm3aBLjjmE3Ie3v99dcDLhMvGxkyZAjgMhwrVaoU87jcBsoUBPjtt98A5woywiVyRX8yTXELjuhLrROzUkFllZk5cybglrj6If/8888lHXJK+Chq9PjjjwPuAqoC8VEq+pMt+C76kwnK2jEsCAtNGIZheMYUcRm6Gtv8sg9TxNmPKWLDMIwswE7EhmEYnrETsWEYhmfsRGwYhuEZOxEbhmF4JiXXhGEYhpF+TBEbhmF4xk7EhmEYnkmp1kRpN1vb/LKPsjQ/KP1zLO3zS4QpYsMwDM/YidgwDMMzkSyDWdZQU8cXXngBgHLlygGuKLlhGKUbU8SGYRieMUXsETV37Nq1KwBVq1YFYNasWd7GZBhG5jFFbBiG4RlTxBlml1124emnnwbgkEMOAVzLcrVM6tOnj5/BGYbhhbSciNUDq2vXrkEH4gMPPBCAHXbYAYAzzzwTgNdffx2AH3/8MeH7qZ2Q2gu9//776RimV7QhN2bMGFq2bBnz2LBhwwA3z1WrVmV2cCUgJyfXIjl16lQAjj322KDj7w8//OBtXEby9OjRA4AOHToA0KxZMxo2bBjzHHWEVmd19bQr7Wy//faAO2/VrFkTgP/85z8ALFu2LC2fY6EJwzAMz6SlVdItt9wCwNChQ9Mzqv/n33//BVxXZ6ku3abjapSprB6FId56663gPqnJ7t27A25e6STs+VWsWBGAL774AoBatWrRr18/AO677750f1w+ylpWVjrmWL16dcAdH6nctWvXAvD2228Hz23Tpg3glOHnn38OEKx60oHPYyiFW6NGjZj716xZA8CRRx4JuK7e+p4ffPDBAPz+++9FfoZl1hmGYWQBaYkRn3zyyQkfU7zz448/LvQ9dKVp2LAhO+64IwAHHHAAAE2aNAFg5MiRMe+VrvhMmCg2/NhjjwFOBYP7uykWno1s2LABgK+++grIVcTx6qK0MmTIEAAqVKgAwD777AO4/RAhFdm4ceMMji4xShyqU6cO4Fa0o0ePBmD16tXBcxs1agTAe++9B7jv84gRIwC47rrrwh9wCdH5Y9CgQUBsopTmU7t27ZjXjBo1CnDKX79b7W3pmKcLU8SGYRieSYsi7tixI5B7dfnyyy9jHpNi+umnn5J+PzktPvnkEyD/1erEE08EYPbs2cUbcAbRjrTm8Pzzz3PeeecBhTtHso177rkHyI0pShmWJlq3bg3kqiv9u3PnzkDsKgecHVHUr18fcHsd6YyvpkL79u0Bt9KcNm0a4Fw7BSE1f8cddwBw5ZVXAtC7d28gOxRx27ZtgYJtoZs2bQJgypQpMc+9/PLLY56nY/rQQw8B6Xc2mSI2DMPwTFoU8ddffx1zW1KOP/54IL8S1tXr3nvvTcvnhIl2nps1awa4ePbgwYNLlRIWiiECnHbaaQBcdtllQGqrIV/stttugHOu1K1bN+bxKlWqALnuASngDz74AIDmzZsX+t5bbbVV8FqflC+f+3NfunQpAI8//njSr50+fTrgFPG2224LQOXKlQFYt25d2saZLq655hoALrnkkpj7J0+ezP/+9z8g19cPBP/X7/XFF18EnMNEj+vvkG5MERuGYXgmMinO2oW866676NmzZ4HPOfTQQwFYtGhRxsaVKieddBJAkD2n2NKTTz4JEGQellZycnKCY6lY/sSJE30OqVDatWsHuFXWHnvsUeRrFOP99ddfAaea5EmV53T33XePeZ1ixL6YO3cu4GLE2r9JBq1GxS677ALAGWecAcCECRPSMcS0ohXIdtttB8Dy5csBuOKKK/Kt0urVqwfA8OHDAecr/uOPPwCnrsP6/ZoiNgzD8Ix3RazMFbkLevXqFTz2999/A87/px3cKCLv8xFHHFHg48rUKaz+woUXXgjkV2XpzlgMk7yOgXR7LcPg0ksvBRIrYSlBxbsXLFgQeN6FdtB1/OKVsPYH9B33RUnU3DfffAPA4sWLAeeJliMkiiiee/TRRwNuJTNq1CjOP/98wMX+b7vtNgCOO+44wHmplbswfvz4UMdqitgwDMMz3hSxcrVfeuklwLUHyovU1XfffQfA5s2bMzS61NHYVHVOO+Wql/Hmm2/me83gwYNj/j9w4EAgf4skZXBJaZVG10WmUaUx1QCJR985qdj58+cX+Z7xSlgoc1Ix5WxEq9N//vnH80iSR3tJqhwnRdy2bdvAU3377bcD+R1a1157LeCaN4SNKWLDMAzPeFPE8poWpISFYozKoFO93ueeew6AGTNmAK6guk+UbaUYsZSwlFVeNSSvop4rd4HQTq3iyaoNq5hXt27dALcLbKSOVhmqHifk/5YiKkwJ77TTToCLQbZq1arA93r++efTMGK/bLPNNoDzD4tkqo/5QvH9eI9zzZo1eeqppwCXFanV9/333w/AzJkzMzVMwOOJWF0qlA570EEHAc4KVBAtWrSIub366qsBl36p4iUrV64MYcQFo3TsvfbaK+b+FStWAPDII48AzkTfoEGDwGAuq5tO0grT3HrrrYDbSHjttddi/h9lcnJy8qX4RpFJkyYB7vumQueyY6k5QWEoVf3666+PuV8bWhIbybxX1FGBoPiC8SogFI/+rk2bNg1sp7Jwxm92hk0ygkUXSyV4fP/996GOKR4LTRiGYXjGmyLWsk12EQXLq1evHpjFVSby7LPPBvIXV9GG2MUXXwy4jbKjjjoKcOGBMDn88MMBF/QXShBQURTNacyYMRx77LGAW9ap+IpsarIEySSv57366qtAtEMS2aCGgWBpqttUUCF1lYIU2sjScctmJaxQhDYgDzvssAKfp7nGp3urI/kee+wRfH+VNJHXohomCnsqBBh//gAX9tQx9YUpYsMwDM94T+gQ2tTSLcCcOXMA17hP9i5Z3+LRhpmUpWLGYbL//vsXeH98eUDFxPM2DlWM+I033gAKbqcELgaeTYkdUHQzgGxFGznx6l+JR4o/Rw2l+u68886AU6/63qkEJLib/kvpAAAdQElEQVRNuaKK2evx+P2LBx54AMhVnNoDyXQjBxU10sq6oNVaVFZwpogNwzA8ExlFXBiPPvooAE888QQAr7zyCpDfLiQUi8oESm1W/Cm+7ZGsatp1zsnJCaxTUsKJ2inpeVLE2Ua6yqJGiRtvvDFfso7Q8YwKUsAqWKM4qNofJWLdunVBXFdxb5XQFGo8qhjxwoUL0zPoEqCiSypa36VLF8CpXo3xo48+Cp6j1YFvTBEbhmF4JisUsdDVWTu0iRRxfLumTKCrbqKYk9TTli1bgriy4uGKx3377beA2+WVt9Xwj5KLDjjggJhjCa7YjxqoRgXFspXOqwQHOQX0fdMqTo8vW7YsSCZSoS2t2lT8R06l9evXhzuJFJBbKn5/RsXsx44dC0CnTp0CRey7NKkwRWwYhuGZjClitaLp27cv4K608tAmg3yBTZs2LfBxKWYV+cgEUhPx2XLaiVaMWBl4QFD4XrFg7SorlldaivrIi5rNKAW6e/fugFOX4NoqaQ8jE771VFBhIylfuQeKaqxQvnx5br75ZgBq1aoFuGxVZQtGSQm3adMGyG0qkReVDtCe0q677grE+r8z7eRIhCliwzAMz4SuiHUVUk76fvvtB7iCKcmgrDTFpfL6HfOyZMkSIL8PN0xUHlBtZ6SgVCymMJ9ifGadfNOlBWUQZqqUYDrRCkYZkqecckrwmMqXKuYYNSUs9N1bu3YtUHRxLO1VPPnkk0HGq+LGKjQVBXdEPFqlyMss98qsWbMA2HrrrQHXlLhKlSrBalRNQX1jitgwDMMzoStieWClhIWqlakS08aNG4PH5H9UGxsp4bxxVnAxVilLZTZlEjk4Tj/9dMCNVXGreCZPnswnn3wCwIcffghEz39aHH755Rcgt/JYUdlY2YBio3mVMOR6o+NjkVFF7iHtUyjjr1q1akCunxacE0L7HA0bNuTdd98FoH///kC0G/bGu1h0KyXcqVMnAO68804gt22ZfNBht0BKFlPEhmEYngldEatimHZbhWJNUoV5PbOK9ajtdyKkhDt37gz4VZbyZuq2rPHXX38BsQ0qFbvLphixss6U1SikLo855piMj6m4aC6ql6xaJcoMVEF78eyzzwK5c09UZziKxGfHKe778ssvA/kb+vbu3TtoLhEVTBEbhmF4JnRFrKuSKiFp91UUpXrzIp+w4s6qJat4luGfRYsWBXWhK1Wq5Hk0qXPVVVcB0LVr15j7peqjXAs6EZqTbksbcksJxfW1h7R69WoA7rnnHsD5iqNE6CdiGaaVUqjljyxoWvLl7dumZA+hVkG6P8obB2WdkSNH0qRJEyC1ZB3faIOxcuXKMfdrg0vfQSN6TJ48GXBp6LrgqMelzjnxzRuihIUmDMMwPJOTSmHknJycaFRRTiNbtmwJ+qfY/LKPdM1PKb3apFMIQkkpmW54KfLOD+wYZiPxx7AgTBEbhmF4xhRxGboa2/wSoxKKL774IuCKiscX+s80poizH1PEhmEYWYAp4jJ0Nbb5ZR+miLMfU8SGYRhZQEqK2DAMw0g/pogNwzA8k1JmXWmP39j8so+yND8o/XMs7fNLhCliwzAMz9iJ2DAMwzN2IjYMw/CMnYgNwzA8YydiwzAMz4Rejzgd1K1bF4CbbroJcK2R9t9/fyB//WLDMIxswhSxYRiGZyKtiA877DCAoJGhmgKq5YlauBvZQYMGDQCYMGECAGeeeSYAP/30k7cxpZM2bdoArmGumnTqfp/NbY1oE8kT8XHHHQfA9OnTAffDveKKKwDYsGGDn4GVUnbYYQcgt8ecummH8TdWkfVWrVoBcM455wAu5KSehNlGr169ABg4cCAA//77b8zjt912GwAPP/ww4IREts63LDBs2DAgt/UXwC233ALA5ZdfHsrnWWjCMAzDM5Erg1mvXj0++ugjAObNmwc4JRWvNNJBWUqvTDS/66+/HshVAZdccgkQTqPFww8/HIDXX3895v5GjRoBsHTp0pTf0+fxkxLu0aMH4JS+UGgi/ntbr149ILmO0L5TnPfcc08ABg8eDMD5558PQPnyuYtpdWc/44wziv0ZUfoNanWo1li77LILAH///TcAF1xwAQD3339/0u9pKc6GYRhZQGRixNtuuy0A9913H5988gkAp512GhCOEvZF1apV6dq1KwDDhw8HoGbNmjHPufLKKwEXO80kV199NQDffPMNkN5WQbvuumva3ivT7LjjjgA0a9aMBx98EIDq1asD7rsrZKeUItYmZTbRu3dvAO644w4AvvrqKwDOPfdcAPbYYw/AfV+uu+46IHutpFL4/fv3B5wSFjIGvPPOO6F8viliwzAMz0RGEStO2bJlS+rXrw/AunXrfA4prRxyyCFAbuz14IMPBkDx+fg4vf4WUlJSJ5mgUqVKAIHq69ChAwDvv/9+id/z4osvLvDxU089FfCzAiiKTp06AdC3b18g9++RKPYrRo8eDThFfO+994Y9zLRQoUIFAIYMGcKIESMA5/jQnNauXQtA8+bNAaeIf//994yONd3o95noO3jeeecB8Nlnn4Xy+aaIDcMwPONdEW+zzTYAdO/eHcjdUf/hhx98DimtKI4oVbTPPvsEiSkzZ84EXBy2Z8+egFOIukpLqfz111+hjHHZsmX57qtcuTIA1157LeCOz5o1a1J+f7kEtBLIBjTfyZMn53tMSjcROTmxm+RFPT8qaOV1ww03cNFFFwFw9913F/hcrZRWrlwJwI8//piBEaafOnXqAHDXXXcV+LiSc+KdPukmO74hhmEYpRjvivjSSy8FXBxR2XOlBandffbZB4CXXnop8EXHo53pdu3aAbD77rvHvFb+6nTz0EMPAbnuDcX8RMeOHQHo0qULkOtqSRWpJjkxVMRJPPnkkym/Z1hICcstoDjwn3/+CeTunstrWrVq1ZjX6jna26hSpUrMe0QVzUN7E9OnT2f8+PEFPle+YmVFZjvPPfccAPvuu2/M/TqGio1v3Lgx1HGYIjYMw/CMd0WsWNP8+fMBWLhwoc/hpJ34K2kqvlxdlX/99de0jimezZs3A7lxMhXiUVxXKKNoxowZAKxatSrp9995552B/Eo4SsgdoZhwvIp99913gdzVijLq4t0Q8oXrb6TnRRV5Z/Xbk1e2f//+CetgTJkyBXDH8tZbbw17mKHSuHFjIL9zady4cQC8/PLLGRmHKWLDMAzPeFPEqjsgZ8B+++2X8LkqIyi3weLFi8MdXBrRDrpu16xZE2Ri7b333oBTTgceeCAAP//8MwCnn346kLkd6d9++y1QR/GKWMdHGVWJFHGFChWC7CshF0gU0d9eMWGheK+U8KBBg/K9VjF7qej4uKqqB8qDHDXXyCmnnAI4v3rbtm0BWL16db7n6ruo3+v69esBGDNmTOjjDAP5o/W7lCKWS0Lx8kxhitgwDMMz3hSxdqeXLFkCwLfffhs8JpWi+NNOO+0EwKZNmwAYOnQo4Oq6Rpn4GNTFF1/MkCFDAKeARbdu3QCnpHygXPqzzjqrwMcPPfRQABYtWgS44v26rVSpUlAroyh07IvjTU4XV111FQDbb799zP033ngjUHCm1VtvvQXAnDlzgMQNCqQa9b2NGjrGqjT29ttv53uO6oNoxSBPtPzF2dic4Z577gn2BPS7/PjjjwHXrEArokzh7UR89tlnA658nr6sFSpUCCxUWuK++OKLgCuHqfTbr7/+GnAdPKKIlvCyPLVo0SLfckhF2MNKn0wF2dNat24N5C9vOHbs2JjbeLbaaquk7VqyDOlHkUppwZLSrFkzwB0XnWDKlStX5GtTLdep4x21xA5ZE5XOrFKP4BJ6nnrqKcAlJqlJw80335yxcaYLhYY6deqUrwDVpEmTABf+zDTR+mYYhmGUQTKuiLVUl3Um3ibTvHnzQOHGL9GfeOIJwG30qZ1JlBWx5qtNjt133z2Yh3j66aeBaChiobCQNmmS5d9//81nBSoK/W0yoYibNGkCOKWnsFcYSRdKUlKKelQSO4466qiY/yvVXnTs2JGJEycCULt2bcCtAmTRy8aCXFqF77bbbsF9Co+ls9xrcTBFbBiG4ZmMK+L42Ex8IenFixcXudkjm5AKyGcDCxYsAJwiy4s2hkoDS5cuDRTx7NmzAYKGpIpF+kTFXaT0wkT2sKjZ1rTBpg2padOmAS5eXqNGjWDPRvFtbYzrWGYTKmDUp08fIDZ5o3379gCsWLEi8wPLgyliwzAMz3hPcY5PVkimwHQ2l8ncb7/9iiwsng3I9P/dd98BLqY8derUfM+VQyEKijgRKj6VDtQMVS3YhcqNZtoaFc+nn34KuGLnUopKUJk6dWrgilFDAMWMswklH2l++t1t3rw5SE/3rYSFKWLDMAzPZFwRx6f8Fgd5XLOxPcvGjRsDJaxi02EVfC8JKln58MMPA67Ii3aZFTOUuioJKvwkB4OPBI9UihglQkpYO/DVqlUDXBlQxYyjkgShY6tb/SbvuOOOoHnmySefDPhX8amg9Pxnn30WgIYNG8Y8fvvtt3PZZZdlfFyFYYrYMAzDMxlXxIkaZibD1ltvDbjY1iOPPJK+gYWM1FKfPn2C7B25PwpqVeQb+UTlvQyTWrVqAc5vGyaJstyUrSl1mAzyCes1J510UszjWlUcf/zxgEsljipaaQ4YMICRI0cCJWsa6wsp4HglLKSUo4QpYsMwDM9kXBEre+ynn34CXPGfRK1ZwClhPUcN/xIVpokSapejehm1atUK4lM+i/tkErVg1zHPm9mUF/mpVWMkUXHyknDDDTcALktTx0fMnTsXcCs2xXulZi+99NJAVUvByyesmiGahzImo66ExWOPPQbkOgniHR/ZRHwLK6E9mShlsApTxIZhGJ7JSSVWm5OTk3pgNwFqvaPmfCoN+eijjwY79E2bNgVcfrt2blWFLR0F07ds2RLYN9I5PyH/pbyMU6dOpUePHun+mISEPb9UaNmyJeCUonbm45FK/eOPP4p8z+LOT/FQ1ZzQZybj8Y5/zhtvvAHkdyGkg7zzg3COYYsWLQBXBnPQoEFBlbVMkO7vqPZc5CMWXbt2BTK/Eo0/hgVhitgwDMMz3hSxiFfG22yzTfCYfMKqD6D4Xjp9t2Epxnbt2gEuxij11L1794xWeoqSIhZSYLNmzQJcrVuh6mBSmoVR0vnJsdGvXz+AoM5JYYpYvuB58+YBLqYdRh2GMBWxWnZJCcvH3aRJk6RWI+kiXd9RVTpULFix4muvvRZw7Y+K49gqCaaIDcMwsgDvtSaUoZUNbY+SQY6O+JrDPXv2BPzXPY0C8qYOHjwYgEsuuQRw1doy6V3VPoO6wsj7q3Zc8n+rSuDo0aODzjBqtJqt9O7dG3B7MbrNpBpOJ6prrSpyQpXkMq2EU8F7aMI36VoWbbfddoAr9NK/f3/AbQZpoyDTRDE0kU7K0vwgvXOUjUsnqoMOOggIxzZYGOk+hsuXLwegYsWKgCt1qT6LmcZCE4ZhGFmA99BEaUGdp88//3zAbYAoJGEYUSN+MyvTSjgs9txzT99DSBlTxIZhGJ6xGHEJ41NKb1Us+IEHHgAICk/7LmJflmKopX1+UPrnWNrnlwhTxIZhGJ4xRVyGrsY2v+zDFHH2Y4rYMAwjC0hJERuGYRjpxxSxYRiGZ1LyEZf2+I3NL/soS/OD0j/H0j6/RJgiNgzD8IydiA3DMDxjJ2LDMAzP2InYMAzDM5Eu+jN16lTA1Rnt1q0bAO+++663MRmGYaQbU8SGYRieibQiVjk7db2YMmUKAPvuuy8Af//9t5dxFYcuXboAuX3C1LPtoosuAmDu3LkA3H///QAsWbIEgIULF2Z6mIZheCCStSbUBlstabbeeuuYx1V5f+PGjSX+rHR7GNWpo2HDhoBrWKiGmHmboybi22+/BeC1114D4LLLLgNg3bp1AGzevDnp8ZQlj2Y656fj2LFjR8C1UmrWrFnCljt9+vQBYM2aNTH3L126FIBPP/005XEUx0fcqVMnAAYOHAjAkUceqdfqPQt83cyZMwGYM2cOL730EgDVqlUD4MsvvwRg/fr1Kc6gaMrSdzQRFpowDMPwTCQVcZMmTQD45JNPYu7XFVvL/MJanidLSa/G+++/PwBHHHEE4BTUcccdV+KxxaNOCk8//TSQnMKKktqoXbs2AO+88w7g/lbFUYqiuPPTikWrjXi06jr11FOLPTaxePFiAE455RTAqctkSFURd+rUiYcffhiA7bffPrWB5uGrr74C3N/h119/BeCvv/6Ked7FF18MuI40xSFK39EwMEVsGIaRBURus658+fIMGzaswMcee+wxID1KOF1ICd91112FPu+7774DCo/v7rbbbkDuhl5BKE75v//9DyiZkiwpDRo0AODPP/8E3PwKY/z48YBTVb///ntIoyual19+GYBatWqF/lmNGzcG4L///S8AkydPBmDQoEFp/6waNWqUSAmL+vXrx/w/0d/piSeeAKBz584AvP/++yX+7LKIKWLDMAzPRE4R33777Zxxxhm+h5Eyil9rx/rnn38G4L777gNg9OjRQOG7zlJIt99+e2jjLClSPlJ1UumFjVkJOe3atQNg1KhRACxfvjy0cRaFlJxinPH89ttvgHO99OvXD3ArgeJQqVIlANq0aQPkKmXFj9PFuHHj0vp+RVGzZk0A5s+fD8Arr7wCQPfu3YH8DpKoUq5cOQD22muvAh///vvvAdi0aVMon2+K2DAMwzORUcR9+/YFnBczW1Dc+pFHHgHgiiuuAFzsdNmyZUm/13vvvVfo43/88QfgdrB9cOaZZwJuBZCMetcqoXz53K+bOl77RCsVxa3j+eeffwAX+5ZTZejQocGcp02bBrjEo6pVqyb12VKRNWrUKM7QC2XEiBH06NEj5r4FCxYArsO4OOiggwD32xP16tULFGKy6NgeffTRgIspF1cRn3jiiQA8++yzxXp9QVSuXBmADh06cPbZZwNQoUIFwOUqaM8nnhEjRgBwww03pG08eTFFbBiG4RnvPuLevXsDMGHCBCD3CqXU3ubNm8c897TTTgPgySefTNvn+/Qw6ip84403As6zqszCeAYMGAAkVnEFke75/fTTT4BbAVx66aVFvka+VsUNmzVrBsDHH39c0uEUe35S9o8++mixP3vvvfcGXOr9wQcfXOjzN2zYAEDPnj0BmDFjRpGf4aNDR//+/QP/sBg+fDgAO+64Y1LvIeWoPYTCKOgYKrOxONmzO++8MwDt27cHnGe8devWQKzq1bnmmWeeAeD4448H3GpBaIUb/3dJBvMRG4ZhZAGhxYi1Q9y0aVPA7Ta3bNkScOp2p512inndoEGDeP755wGXo1/aUO7/4MGDgaKz8L755hsgOQUVFopryuOcykpKmZCKcUtd+CRZJSxPruK5iguDiznGe27jkVPmvPPOA/wex2QoaMU1adIkwP097rjjDgCOOeYYwP3eRYcOHYDkFHFBlKSOzOuvvw5Ao0aNgPw1NnJycoKYf//+/QFYuXIlAD/88APgFLFeq1VdWJgiNgzD8Exoinj33XcH3E5tvP9SPs17770XgFtuuQXIdRnotaWR3r17M3HiRIAid6avu+46wDkU5E32wYoVKwC3C65YmarJFeavVLxv0aJFgKuql8xrfSHlJ3eFVnCpoO+4nEBRV8KFoSxIrWpUGTDRak6K2QdSvlLVH330EQAjR44EcjNS5QtWlu4ll1wCwDXXXBPzXlqNqs5LWJgiNgzD8Exoivjzzz8HXHWy+DiaausmU6NApCOHPtNo/ieddBIAV111VUIlrNipYuTKXkvFixw206dPB2DIkCGA26FWfRApiIKoW7cu4LKv5BZR3YcoIXdAcZSwUNZeNivheBQXT+TckWdZzQ58oO+m9pgK22uSX1mKV3sg+s2pjrjcQmERekKHlp2pFKjRMkhL8V133RVwJ7OHHnoojSNML7Kkydokq129evWC56jwT3yHEZnGb7311tDHWVxuuukmwM1HljudsKZNmxZcUHTi1YaHfsQy6UfxBCy0rFURG3VVSQVdaBSS0W02ogtuot+elvh6XJtfPnjhhReKfI5CEfrN6QSskIU2GzMlgiw0YRiG4ZnIpDjnZdWqVYBrGSRF7HO5kywqNJ4ouD9v3ryg4EwqiRlRQZt18UpYRc/zhqBkTdTmiVY0c+bMycxgS8Dq1asBl/hx4IEH5nuOQkfxrbzELrvsAhAUscomRazwWa9evQC34Sj7qVBJU222a/M9ypxwwglBwomOnUJqai+VaeusKWLDMAzPeE9xLgy1Xzn00EOBaKY4awNRMVMVtIkvpyc136NHj9AD/3nxmcKt4jNSjip8/8svv6TtM3zOT/a7O++8E8hfPEdoT0CJPCoZmQw+UpzBKWAlciTizTffBNzcikOmjqGsdlOnTg0SUGQWUInWMJSwpTgbhmFkAZGMEccj1e5zJzYRiqElapWkdEsVVPfZHijTyDVRWpEjSKVQtWKrUqVKzPMUb5V7JKrISXDBBRdQrVq1Ap8j26lKF0QxGSceKWElRpUrVy5IKlJhIN8WUVPEhmEYnvGmiBVTjS+mvWHDhmDH+rbbbgPcjqwKr+hWabbaAVXsOJ3FpBOhgiKJykC++uqrgCv9mIwSVoFxxZ01L90fz/r164NEipK0M083ip2ecMIJgPOQS02VNhRnjPeFC6W0F1X4P9McdthhAFx44YUANGnSBCi4DKsSNfRbTCURyxf6/qnAU95EKj3mWwkLU8SGYRieCV0RqxWJ4oVqwnjuuecC+Qst//XXX0HZwHi1LMWrdvJ6b8XklIkXpiJWUXOVQ0xUoEi7r/LV5o1vq7BIfKqzPKuJFHA8vXv3jpQSFvIPH3DAAQDcfPPNQMlKG0YRreqmTp0KQPXq1Qt8nr7P8txGBX135QHPi4r7yOuujMq1a9dmaHTFR4peK0o5JPQbvOCCC/jiiy/8DC4BpogNwzA8E5oiVlaRPJZdu3Yt9Pny1m7ZsiVoMa7ydckiv2qYSOmqDKBqSsQjxa+d9Lzx0dq1awMl30VXg8aoEV8aMQrNQiHX65rI3aKi7fEeZ5WyVAZWxYoVg6IyHTt2BJJfwUQNlVktCGXIzZ49G4hGMf+i0IpEPm0pfsWzzzrrLADeeOMND6MrHFPEhmEYnglNESu/PpESnjVrFuAqjekqlmjnOSoo3qedZsWnE5VLVLw0viVUcVDbGdXiiG+PHhXim75+8MEHnkYSS5UqVdh3330LfEwZYvEoBi+1Fd/goDBUiWzevHkpjDJ8pO61ai2Iiy66KOZWpTzVADUefRflrpD3P5M+48aNGwNOCf/zzz+AOxdFcT9FmCI2DMPwTGiKWFfQ3r17A67VjiqPPfjgg2F9dEbQlV4eRXmbi5Nzrxqo3bp1A2DJkiUFPk9eZNV+jSJNmzYNGjKmUlMhqshrmwxyFOj4jB49GnBNEqKCYviqwZsMygxNhBw/Qp7p4cOHZ6xqov7+cnyo3nWUlbAwRWwYhuGZSFdfywTpqvwkr6IydurUqQM4L6OYNGlSvnikaqG+++67xf34hGS6OlmbNm2CrEI5FAYPHhza56Uyv2bNmjFgwADAKTh50YuDWqyrutrQoUOB9Hptw6y+JheBHCOKrdasWTPl91LtcOUFaJVXt27dYLWYiHR/R+VK0nHwndGZTPU1OxF7LKOYCTI9v1tuuSUohCRrX5iFjoo7P500lWxSFCr0n7e4u6xdOhGHQSbLYKodlAr6gAtjqKh/PFdeeSUATz/9NOA2pSUuTjjhBO6///5CP7cs/QYTYaEJwzAMz5giLkNX40wpYhXyP+KII8L+uDJ1/KD0z7G0zy8RpogNwzA8Y4q4DF2NbX7Zhyni7McUsWEYRhZgJ2LDMAzP2InYMAzDMynFiA3DMIz0Y4rYMAzDM3YiNgzD8IydiA3DMDxjJ2LDMAzP2InYMAzDM3YiNgzD8IydiA3DMDxjJ2LDMAzP2InYMAzDM3YiNgzD8Mz/AW+i0FW53AuuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show a batch\n",
    "mnist = MNIST(batch_size=30) \n",
    "show_images(mnist.X[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakyReLU\n",
    "In the cell below, you should implement a LeakyReLU. See the [class notes](http://cs231n.github.io/neural-networks-1/) (where alpha is small number) or equation (3) in [this paper](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf). LeakyReLUs keep ReLU units from dying and are often used in GAN methods (as are maxout units, however those increase model size and therefore are not used in this notebook).\n",
    "\n",
    "HINT: You should be able to use `tf.maximum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    # TODO: implement leaky ReLU\n",
    "    return tf.maximum(x, alpha * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your leaky ReLU implementation. You should get errors < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum error: 0\n"
     ]
    }
   ],
   "source": [
    "def test_leaky_relu(x, y_true):\n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        y_tf = leaky_relu(tf.constant(x))\n",
    "        y = sess.run(y_tf)\n",
    "        print('Maximum error: %g'%rel_error(y_true, y))\n",
    "\n",
    "test_leaky_relu(answers['lrelu_x'], answers['lrelu_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Noise\n",
    "Generate a TensorFlow `Tensor` containing uniform noise from -1 to 1 with shape `[batch_size, dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"Generate random uniform noise from -1 to 1.\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size of noise to generate\n",
    "    - dim: integer giving the dimension of the the noise to generate\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor containing uniform noise in [-1, 1] with shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # TODO: sample and return noise\n",
    "    return tf.random_uniform(minval=-1, maxval=1, shape=[batch_size,dim])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure noise is the correct shape and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_sample_noise():\n",
    "    batch_size = 3\n",
    "    dim = 4\n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        z = sample_noise(batch_size, dim)\n",
    "        # Check z has the correct shape\n",
    "        assert z.get_shape().as_list() == [batch_size, dim]\n",
    "        # Make sure z is a Tensor and not a numpy array\n",
    "        assert isinstance(z, tf.Tensor)\n",
    "        # Check that we get different noise for different evaluations\n",
    "        z1 = sess.run(z)\n",
    "        z2 = sess.run(z)\n",
    "        assert not np.array_equal(z1, z2)\n",
    "        # Check that we get the correct range\n",
    "        assert np.all(z1 >= -1.0) and np.all(z1 <= 1.0)\n",
    "        print(\"All tests passed!\")\n",
    "    \n",
    "test_sample_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "Our first step is to build a discriminator. You should use the layers in `tf.layers` to build the model.\n",
    "All fully connected layers should include bias terms. For initialization, just use the default initializer used by the `tf.layers` functions.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected layer with input size 784 and output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with output size 1 \n",
    " \n",
    "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the scores that each of the `batch_size` inputs is a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, 784]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score \n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        # TODO: implement architecture\n",
    "        fc1 = tf.layers.dense(x, units=256, activation=leaky_relu, use_bias=True)\n",
    "        fc2 = tf.layers.dense(fc1, units=256, activation=leaky_relu, use_bias=True)\n",
    "        logits = tf.layers.dense(fc2, units=1, use_bias=True)\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure the number of parameters in the discriminator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of parameters in discriminator.\n"
     ]
    }
   ],
   "source": [
    "def test_discriminator(true_count=267009):\n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        y = discriminator(tf.ones((2, 784)))\n",
    "        cur_count = count_params()\n",
    "        if cur_count != true_count:\n",
    "            print('Incorrect number of parameters in discriminator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "        else:\n",
    "            print('Correct number of parameters in discriminator.')\n",
    "        \n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Now to build a generator. You should use the layers in `tf.layers` to construct the model. All fully connected layers should include bias terms. Note that you can use the tf.nn module to access activation functions. Once again, use the default initializers for parameters.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected layer with inupt size tf.shape(z)[1] (the number of noise dimensions) and output size 1024\n",
    " * `ReLU`\n",
    " * Fully connected layer with output size 1024 \n",
    " * `ReLU`\n",
    " * Fully connected layer with output size 784\n",
    " * `TanH` (To restrict every element of the output to be in the range [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "    \n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 784].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        # TODO: implement architecture\n",
    "        fc1 = tf.layers.dense(z, units=1024, activation=tf.nn.relu, use_bias=True)\n",
    "        fc2 = tf.layers.dense(fc1, units=1024, activation=tf.nn.relu, use_bias=True)\n",
    "        img = tf.layers.dense(fc2, units=784, activation=tf.nn.tanh)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure the number of parameters in the generator is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of parameters in generator.\n"
     ]
    }
   ],
   "source": [
    "def test_generator(true_count=1858320):\n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        y = generator(tf.ones((1, 4)))\n",
    "        cur_count = count_params()\n",
    "        if cur_count != true_count:\n",
    "            print('Incorrect number of parameters in generator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "        else:\n",
    "            print('Correct number of parameters in generator.')\n",
    "        \n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Loss\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses.\n",
    "\n",
    "**HINTS**: Use [tf.ones_like](https://www.tensorflow.org/api_docs/python/tf/ones_like) and [tf.zeros_like](https://www.tensorflow.org/api_docs/python/tf/zeros_like) to generate labels for your discriminator. Use [tf.nn.sigmoid_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) to help compute your loss function. Instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gan_loss(logits_real, logits_fake):\n",
    "    \"\"\"Compute the GAN loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: Tensor, shape [batch_size, 1], output of discriminator\n",
    "        Unnormalized score that the image is real for each real image\n",
    "    - logits_fake: Tensor, shape[batch_size, 1], output of discriminator\n",
    "        Unnormalized score that the image is real for each fake image\n",
    "    \n",
    "    Returns:\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \n",
    "    HINT: for the discriminator loss, you'll want to do the averaging separately for\n",
    "    its two components, and then add them together (instead of averaging once at the very end).\n",
    "    \"\"\"\n",
    "    # Target label vector for generator loss and used in discriminator loss.\n",
    "    true_labels = tf.ones_like(logits_fake)\n",
    "    \n",
    "    # DISCRIMINATOR loss has 2 parts: \n",
    "    # 1. how well it classifies real images. \n",
    "    # 2. how well it classifies fake images.\n",
    "    real_image_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real, labels=true_labels)\n",
    "    fake_image_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=1-true_labels)\n",
    "    \n",
    "    # Combine and average losses over the batch\n",
    "    D_loss = real_image_loss + fake_image_loss \n",
    "    D_loss = tf.reduce_mean(D_loss)\n",
    "    \n",
    "    # GENERATOR is trying to make the discriminator output 1 for all its images.\n",
    "    # So we use our target label vector of ones for computing generator loss.\n",
    "    G_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=true_labels)\n",
    "    \n",
    "    # Average generator loss over the batch.\n",
    "    G_loss = tf.reduce_mean(G_loss)\n",
    "    \n",
    "    return D_loss, G_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your GAN loss. Make sure both the generator and discriminator loss are correct. You should see errors less than 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum error in d_loss: 0\n",
      "Maximum error in g_loss: 0\n"
     ]
    }
   ],
   "source": [
    "def test_gan_loss(logits_real, logits_fake, d_loss_true, g_loss_true):\n",
    "    tf.reset_default_graph()\n",
    "    with get_session() as sess:\n",
    "        d_loss, g_loss = sess.run(gan_loss(tf.constant(logits_real), tf.constant(logits_fake)))\n",
    "    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n",
    "    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\n",
    "\n",
    "test_gan_loss(answers['logits_real'], answers['logits_fake'],\n",
    "              answers['d_loss_true'], answers['g_loss_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing our loss\n",
    "Make an `AdamOptimizer` with a 1e-3 learning rate, beta1=0.5 to mininize G_loss and D_loss separately. The trick of decreasing beta was shown to be effective in helping GANs converge in the [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) paper. In fact, with our current hyperparameters, if you set beta1 to the Tensorflow default of 0.9, there's a good chance your discriminator loss will go to zero and the generator will fail to learn entirely. In fact, this is a common failure mode in GANs; if your D(x) learns to be too fast (e.g. loss goes near zero), your G(z) is never able to learn. Often D(x) is trained with SGD with Momentum or RMSProp instead of Adam, but here we'll use Adam for both D(x) and G(z). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create an AdamOptimizer for D_solver and G_solver\n",
    "def get_solvers(learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"Create solvers for GAN training.\n",
    "    \n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "    \n",
    "    Returns:\n",
    "    - D_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    \n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Now just a bit of Lego Construction.. Read this section over carefully to understand how we'll be composing the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# number of images for each batch\n",
    "batch_size = 128\n",
    "# our noise dimension\n",
    "noise_dim = 96\n",
    "\n",
    "# placeholder for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# random noise fed into our generator\n",
    "z = sample_noise(batch_size, noise_dim)\n",
    "# generated images\n",
    "G_sample = generator(z)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    #scale images to be -1 to 1\n",
    "    logits_real = discriminator(preprocess_img(x))\n",
    "    # Re-use discriminator weights on new inputs\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator') \n",
    "\n",
    "# get our solver\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# get our loss\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "\n",
    "# setup training steps\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "D_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'discriminator') # not used\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'generator') # not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN!\n",
    "Well that wasn't so hard, was it? After the first epoch, you should see fuzzy outlines, clear shapes as you approach epoch 3, and decent shapes, about half of which will be sharp and clearly recognizable as we pass epoch 5. In our case, we'll simply train D(x) and G(z) with one batch each every iteration. However, papers often experiment with different schedules of training D(x) and G(z), sometimes doing one for more steps than the other, or even training each one until the loss gets \"good enough\" and then switching to training the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a giant helper function\n",
    "def run_a_gan(sess, G_train_step, G_loss, D_train_step, D_loss, G_extra_step, D_extra_step,\\\n",
    "              show_every=2, print_every=1, batch_size=128, num_epoch=10):\n",
    "    \"\"\"Train a GAN for a certain number of epochs.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A tf.Session that we want to use to run our data\n",
    "    - G_train_step: A training step for the Generator\n",
    "    - G_loss: Generator loss\n",
    "    - D_train_step: A training step for the Generator\n",
    "    - D_loss: Discriminator loss\n",
    "    - G_extra_step: A collection of tf.GraphKeys.UPDATE_OPS for generator # not used\n",
    "    - D_extra_step: A collection of tf.GraphKeys.UPDATE_OPS for discriminator # not used\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    # compute the number of iterations we need\n",
    "    mnist = MNIST(batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epoch):\n",
    "        # every show often, show a sample result\n",
    "        if epoch % show_every == 0:\n",
    "            samples = sess.run(G_sample)\n",
    "            fig = show_images(samples[:16])\n",
    "            plt.show()\n",
    "            print()\n",
    "        for (minibatch, minbatch_y) in mnist:\n",
    "            # run a batch of data through the network\n",
    "            _, D_loss_curr = sess.run([D_train_step, D_loss], feed_dict={x: minibatch})\n",
    "            _, G_loss_curr = sess.run([G_train_step, G_loss])\n",
    "\n",
    "        # print loss every so often.\n",
    "        # We want to make sure D_loss doesn't go to 0\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: {}, D: {:.4}, G:{:.4}'.format(epoch,D_loss_curr,G_loss_curr))\n",
    "    print('Final images')\n",
    "    samples = sess.run(G_sample)\n",
    "\n",
    "    fig = show_images(samples[:16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train your GAN! This should take about 10 minutes on a CPU, or less than a minute on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [96,1] vs. [128,1]\n\t [[Node: gradients/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/logistic_loss/mul_grad/Shape, gradients/logistic_loss/sub_grad/Shape_1)]]\n\nCaused by op 'gradients/logistic_loss/mul_grad/BroadcastGradientArgs', defined at:\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0457d9a6cd9f>\", line 33, in <module>\n    D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 399, in minimize\n    grad_loss=grad_loss)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 492, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 379, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 881, in _MulGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 671, in _broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'logistic_loss/mul', defined at:\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0457d9a6cd9f>\", line 30, in <module>\n    D_loss, G_loss = gan_loss(logits_real, logits_fake)\n  File \"<ipython-input-12-6c4ad3fc29b0>\", line 23, in gan_loss\n    real_image_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real, labels=true_labels)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 181, in sigmoid_cross_entropy_with_logits\n    relu_logits - logits * labels,\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 971, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1198, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4689, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [96,1] vs. [128,1]\n\t [[Node: gradients/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/logistic_loss/mul_grad/Shape, gradients/logistic_loss/sub_grad/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [96,1] vs. [128,1]\n\t [[Node: gradients/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/logistic_loss/mul_grad/Shape, gradients/logistic_loss/sub_grad/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-14ec25d23f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_a_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_train_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_extra_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_extra_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-017e2fde0eed>\u001b[0m in \u001b[0;36mrun_a_gan\u001b[0;34m(sess, G_train_step, G_loss, D_train_step, D_loss, G_extra_step, D_extra_step, show_every, print_every, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# run a batch of data through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [96,1] vs. [128,1]\n\t [[Node: gradients/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/logistic_loss/mul_grad/Shape, gradients/logistic_loss/sub_grad/Shape_1)]]\n\nCaused by op 'gradients/logistic_loss/mul_grad/BroadcastGradientArgs', defined at:\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0457d9a6cd9f>\", line 33, in <module>\n    D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 399, in minimize\n    grad_loss=grad_loss)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 492, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 379, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 881, in _MulGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 671, in _broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'logistic_loss/mul', defined at:\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0457d9a6cd9f>\", line 30, in <module>\n    D_loss, G_loss = gan_loss(logits_real, logits_fake)\n  File \"<ipython-input-12-6c4ad3fc29b0>\", line 23, in gan_loss\n    real_image_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real, labels=true_labels)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 181, in sigmoid_cross_entropy_with_logits\n    relu_logits - logits * labels,\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 971, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1198, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4689, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [96,1] vs. [128,1]\n\t [[Node: gradients/logistic_loss/mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gradients/logistic_loss/mul_grad/Shape, gradients/logistic_loss/sub_grad/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,G_train_step,G_loss,D_train_step,D_loss,G_extra_step,D_extra_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares GAN\n",
    "We'll now look at [Least Squares GAN](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all we have to do is change the loss function and retrain the model. We'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "and the discriminator loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$\n",
    "\n",
    "\n",
    "**HINTS**: Instead of computing the expectation, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing. When plugging in for $D(x)$ and $D(G(z))$ use the direct output from the discriminator (`score_real` and `score_fake`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsgan_loss(scores_real, scores_fake):\n",
    "    \"\"\"Compute the Least Squares GAN loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - scores_real: Tensor, shape [batch_size, 1], output of discriminator\n",
    "        The score for each real image\n",
    "    - scores_fake: Tensor, shape[batch_size, 1], output of discriminator\n",
    "        The score for each fake image    \n",
    "          \n",
    "    Returns:\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \"\"\"\n",
    "    # TODO: compute D_loss and G_loss\n",
    "    G_loss = 0.5 * tf.reduce_mean((scores_fake-1) ** 2)\n",
    "    D_loss = 0.5 * tf.reduce_mean((scores_real-1) ** 2) + 0.5 * tf.reduce_mean((scores_fake) ** 2)\n",
    "    \n",
    "    return D_loss, G_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your LSGAN loss. You should see errors less than 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum error in d_loss: 0\n",
      "Maximum error in g_loss: 0\n"
     ]
    }
   ],
   "source": [
    "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n",
    "    with get_session() as sess:\n",
    "        d_loss, g_loss = sess.run(\n",
    "            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n",
    "    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n",
    "    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\n",
    "\n",
    "test_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new training steps so we instead minimize the LSGAN loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const_31:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"pow_3:0\", shape=(128, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bacc38504170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsgan_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mD_train_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mG_train_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c189c05b383e>\u001b[0m in \u001b[0;36mlsgan_loss\u001b[0;34m(scores_real, scores_fake)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# TODO: compute D_loss and G_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_fake\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_real\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                                                   reduction_indices),\n\u001b[1;32m   1548\u001b[0m                                    \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                                    name=name))\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   4423\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4424\u001b[0m         \u001b[0;34m\"Mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4425\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4426\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4427\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5428\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5429\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5430\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5431\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5364\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5365\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 5366\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   5367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_31:0\", shape=(2,), dtype=int32) must be from the same graph as Tensor(\"pow_3:0\", shape=(128, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the following cell to train your model!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Tensor 'generator/dense_2/Tanh:0' shape=(128, 784) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"generator/dense_2/Tanh:0\", shape=(128, 784), dtype=float32) is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3556\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"generator/dense_2/Tanh:0\", shape=(128, 784), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-685063becfb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_a_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_extra_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_extra_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-017e2fde0eed>\u001b[0m in \u001b[0;36mrun_a_gan\u001b[0;34m(sess, G_train_step, G_loss, D_train_step, D_loss, G_extra_step, D_extra_step, show_every, print_every, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# every show often, show a sample result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mshow_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1125\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 289\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    290\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Tensor 'generator/dense_2/Tanh:0' shape=(128, 784) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(\"generator/dense_2/Tanh:0\", shape=(128, 784), dtype=float32) is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess, G_train_step, G_loss, D_train_step, D_loss, G_extra_step, D_extra_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs\n",
    "In the first part of the notebook, we implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, we will implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where we use convolutional networks as our discriminators and generators.\n",
    "\n",
    "#### Discriminator\n",
    "We will use a discriminator inspired by the TensorFlow MNIST classification [tutorial](https://www.tensorflow.org/get_started/mnist/pros), which is able to get above 99% accuracy on the MNIST dataset fairly quickly. *Be sure to check the dimensions of x and reshape when needed*, fully connected blocks expect [N,D] Tensors while conv2d blocks expect [N,H,W,C] Tensors. Please use `tf.layers` to define the following architecture:\n",
    "\n",
    "Architecture:\n",
    "* Conv2D: 32 Filters, 5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Conv2D: 64 Filters, 5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Flatten\n",
    "* Fully Connected with output size 4 x 4 x 64\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Fully Connected with output size 1\n",
    "\n",
    "Once again, please use biases for all convolutional and fully connected layers, and use the default parameter initializers. Note that a padding of 0 can be accomplished with the 'VALID' padding option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elisim/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Correct number of parameters in discriminator.\n"
     ]
    }
   ],
   "source": [
    "def discriminator(x):\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, 784]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score \n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        # TODO: implement architecture\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = tf.layers.conv2d(x, filters=32, kernel_size=5, strides=1, activation=leaky_relu)\n",
    "        x = tf.layers.max_pooling2d(x, pool_size=2, strides=2)\n",
    "        x = tf.layers.conv2d(x, filters=64, kernel_size=5, strides=1, activation=leaky_relu)\n",
    "        x = tf.layers.max_pooling2d(x, pool_size=2, strides=2)\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 4*4*64, activation=leaky_relu)\n",
    "        logits = tf.layers.dense(x, 1)\n",
    "        return logits\n",
    "test_discriminator(1102721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "For the generator, we will copy the architecture exactly from the [InfoGAN paper](https://arxiv.org/pdf/1606.03657.pdf). See Appendix C.1 MNIST. Please use `tf.layers` for your implementation. You might find the documentation for [tf.layers.conv2d_transpose](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose) useful. The architecture is as follows.\n",
    "\n",
    "Architecture:\n",
    "* Fully connected with output size 1024 \n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Fully connected with output size 7 x 7 x 128 \n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Resize into Image Tensor of size 7, 7, 128\n",
    "* Conv2D^T (transpose): 64 filters of 4x4, stride 2\n",
    "* `ReLU`\n",
    "* BatchNorm\n",
    "* Conv2d^T (transpose): 1 filter of 4x4, stride 2\n",
    "* `TanH`\n",
    "\n",
    "Once again, use biases for the fully connected and transpose convolutional layers. Please use the default initializers for your parameters. For padding, choose the 'same' option for transpose convolutions. For Batch Normalization, assume we are always in 'training' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "    \n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 784].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        # TODO: implement architecture\n",
    "        pass\n",
    "        return img\n",
    "test_generator(6595521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to recreate our network since we've changed our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 128\n",
    "# our noise dimension\n",
    "noise_dim = 96\n",
    "\n",
    "# placeholders for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "z = sample_noise(batch_size, noise_dim)\n",
    "# generated images\n",
    "G_sample = generator(z)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    #scale images to be -1 to 1\n",
    "    logits_real = discriminator(preprocess_img(x))\n",
    "    # Re-use discriminator weights on new inputs\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'generator') \n",
    "\n",
    "D_solver,G_solver = get_solvers()\n",
    "D_loss, G_loss = gan_loss(logits_real, logits_fake)\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "D_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'discriminator')\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS,'generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a DCGAN\n",
    "This is the one part of A3 that significantly benefits from using a GPU. It takes 3 minutes on a GPU for the requested five epochs. Or about 50 minutes on a dual core laptop on CPU (feel free to use 3 epochs if you do it on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,G_train_step,G_loss,D_train_step,D_loss,G_extra_step,D_extra_step,num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INLINE QUESTION 1\n",
    "\n",
    "We will look at an example to see why alternating minimization of the same objective (like in a GAN) can be tricky business.\n",
    "\n",
    "Consider $f(x,y)=xy$. What does $\\min_x\\max_y f(x,y)$ evaluate to? (Hint: minmax tries to minimize the maximum value achievable.)\n",
    "\n",
    "Now try to evaluate this function numerically for 6 steps, starting at the point $(1,1)$, \n",
    "by using alternating gradient (first updating y, then updating x) with step size $1$. \n",
    "You'll find that writing out the update step in terms of $x_t,y_t,x_{t+1},y_{t+1}$ will be useful.\n",
    "\n",
    "Record the six pairs of explicit values for $(x_t,y_t)$ in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    " \n",
    " $y_0$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ | $y_6$ \n",
    " ----- | ----- | ----- | ----- | ----- | ----- | ----- \n",
    "   1   |       |       |       |       |       |       \n",
    " $x_0$ | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | $x_6$ \n",
    "   1   |       |       |       |       |       |       \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INLINE QUESTION 2\n",
    "Using this method, will we ever reach the optimal value? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INLINE QUESTION 3\n",
    "If the generator loss decreases during training while the discriminator loss stays at a constant high value from the start, is this a good sign? Why or why not? A qualitative answer is sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
